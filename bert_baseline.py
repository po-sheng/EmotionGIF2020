# -*- coding: utf-8 -*-
"""final_project_refine.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zAB3BOYAa5LdXZGRXYhJ71eCE-EE7nqZ
"""

######################################
# Run this before running the python #
######################################
# !git clone -b master https://github.com/charles9n/bert-sklearn
# !pip install ./bert-sklearn


import pandas as pd
import numpy as np
from bert_sklearn import BertClassifier
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import SVC
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from sklearn.naive_bayes import GaussianNB, MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from collections import Counter, defaultdict
import json
import sys
import requests

# Phrase
name = "dev"

# Set numpy variable
np.set_printoptions(threshold=sys.maxsize)

# Read data file
train_data = pd.read_json('train_gold.json', lines=True)
categories = pd.read_json('categories.json', lines=True)
test_data = pd.read_json(name + '_unlabeled.json', lines=True)

# Manually split training and testing data
# train_data, test_data = train_test_split(data, test_size=0.1, random_state=666)

# Part 1
train_num = len(train_data["idx"])

numOfCat = np.zeros(6)
catCount = defaultdict(lambda: 0)
catCoOccurred = defaultdict(lambda: 0)

for order in range(train_num):

  # compute number of samples that have different categories
  cat_num = len((train_data["categories"].values)[order])
  numOfCat[cat_num - 1] += 1

  # "catList" each character represents one of the categories. 0 means having this categories in certain doc  
  catList = ""

  # compute most common pair of categories
  if len((train_data["categories"].values)[order]) > 1:
    for idx in range(categories.size):
      if categories[idx][0] in (train_data["categories"].values)[order]:
        catList += '1'
      else:
        catList += '0'
    catCoOccurred[catList] += 1

  # compute categories distribution
  for cat in (train_data["categories"].values)[order]:
    catCount[cat] += 1

# Part 1 print result
# print number of sample that have different number of categories
print("Number of sample with N categories:")
for num in range(6):
  print("\t", num+1, "categories :", int(numOfCat[num]))
print()

# print category distribution
print("Category distribution:")
for cat_num in range(len(catCount.keys())):
  print("\t", list(catCount.keys())[cat_num], ":", list(catCount.values())[cat_num])
print()

# print top 10 pairs of co-occurring categories
print("10 most common pairs of co-occurring categories:")
for idx in range(10):
  first = 0
  print("\t [", end='')
  for cat_num in range(categories.size):
    if list(catCoOccurred.keys())[idx][cat_num] == '1':
      if first == 1:
        print(", ", end='')
      print(categories[cat_num][0], end='')
      first = 1
  print("] :", list(catCoOccurred.values())[idx])

# Write the answer json file
def write_json(dataset, ans):
  dataset_ret = ""

  # Add category key value pair to test, dev
  dataset["categories"] = ans

  # break into each doc
  for doc_num in range(len(dataset["idx"])):
    doc = {}
    for keys in dataset.keys():
      doc[keys] = dataset[keys][doc_num]

    # transfer from dict to str
    dataset_ret += json.dumps(doc)
    dataset_ret += "\n"

  # Write file
  with open(name + '.json', 'w') as fp:
    fp.write(dataset_ret)

# Part 2 first model (majority prediction)
dataset = {}

# convert pandas to dictionary
for key in test_data.columns:
  data_list = []
  for data_num in range(len(test_data[key])):
    data_list.append(list(test_data[key].values)[data_num])
    if isinstance(data_list[-1], np.int64):
      data_list[-1] = int(data_list[-1])

  dataset[key] = data_list[0] if len(data_list) == 1 else data_list

# add new key-value of "categories" to dict
cats = []
for doc_num in range(len(test_data["idx"])):
  cat = []
  for idx in range(6):
    cat.append(list(catCount.keys())[idx])
  cats.append(cat)

write_json(dataset, cats)

# download file from google colab to localhost
# files.download(name + '.json')

# data augmentation by take every category of training data inro account
frame = {}                   # new pandas dataframe for augmentation data
N = 0                                 # Counter for each data row
for doc_num in range(len(train_data)):
  for cats in range(len(train_data.iloc[doc_num]["categories"])):
    frame[N] = train_data.iloc[doc_num]
    frame[N]["categories"] = [train_data.iloc[doc_num]["categories"][cats]] 
    N += 1
train_data_aug = pd.DataFrame(frame)
train_data_aug = train_data_aug.T

# Part 2 second model Naive Bayes
# vectorizer = TfidfVectorizer() # using default parameters
# reply_vectorizer = TfidfVectorizer() # using default parameters
# vectorizer.fit(train_data_aug['text'])
# reply_vectorizer.fit(train_data_aug['reply'])
# train_X = vectorizer.transform(train_data_aug['text'])
# test_X = vectorizer.transform(test_data['text'])
# train_reply_X = reply_vectorizer.transform(train_data_aug['reply'])
# test_reply_X = reply_vectorizer.transform(test_data['reply'])

# train_X.shape, test_X.shape

# duplicate the data belongs to multiple categories
train_y = [categories[0]  for categories in train_data_aug['categories'].to_list()]
# test_y = [categories[0]  for categories in test_data['categories'].to_list()]

len(train_y)

# pred_y = OneVsRestClassifier(SVC(kernel='RBF')).fit(train_X, train_data["categories"])

# pred_reply_y = OneVsRestClassifier(SVC(kernel='RBF')).fit(train_reply_X, train_data["categories"])


# model = MultinomialNB()
# reply_model = MultinomialNB()
# model.fit(train_X.toarray(),train_y)
# reply_model.fit(train_reply_X.toarray(), train_y)
# pred_y = model.predict_proba(test_X.toarray())
# pred_reply_y = reply_model.predict_proba(test_reply_X.toarray())

model = BertClassifier()
reply_model = BertClassifier()

# model options
model.bert_model = 'bert-base-cased'
reply_model.bert_model = 'bert-base-cased'

model.fit(train_data_aug["text"],train_y)
reply_model.fit(train_data_aug["reply"], train_y)
pred_y = model.predict_proba(test_data["text"])
pred_reply_y = reply_model.predict_proba(test_data["reply"])

ratio = 0.4
y = np.array(pred_y)*(1 - ratio) + np.array(pred_reply_y)*ratio


# find the 6 maximize probability categories
top_six = []
for doc_prob in y:
  index_sort = np.argsort(-np.array(doc_prob)) 
  top_six.append(list(index_sort[:6]))

for doc_idx in top_six:
  for idx in range(len(doc_idx)):
    doc_idx[idx] = categories[doc_idx[idx]][0]

dataset = {}

# convert pandas to dictionary
for key in test_data.columns:
  data_list = []
  for data_num in range(len(test_data[key])):
    data_list.append(list(test_data[key].values)[data_num])
    if isinstance(data_list[-1], np.int64):
      data_list[-1] = int(data_list[-1])

  dataset[key] = data_list[0] if len(data_list) == 1 else data_list

write_json(dataset, top_six)

# download file from google colab to localhost
# files.download(name + '.json')

# pred_y = [doc_cats[0] for doc_cats in top_six]
# print(accuracy_score(test_y, pred_y))
# print(classification_report(test_y, pred_y, digits=3))
