# -*- coding: utf-8 -*-
"""final_project_refine.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zAB3BOYAa5LdXZGRXYhJ71eCE-EE7nqZ
"""

######################################
# Run this before running the python #
######################################
# !git clone -b master https://github.com/charles9n/bert-sklearn
# !pip install ./bert-sklearn

import pandas as pd
import numpy as np
from bert_sklearn import BertClassifier, load_model 
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import SVC
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from sklearn.naive_bayes import GaussianNB, MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from collections import Counter, defaultdict
import json
import sys
import requests

# Bert model
bert_model = "bert-base-cased"

# Model file path
textmodel = "dataset/text_model.bin"
replymodel = "dataset/reply_model.bin"

# Phrase
name = "dev"

# Set numpy variable
np.set_printoptions(threshold=sys.maxsize)

# Write the answer json file (test data + "categories" key)
def write_json(dataset, ans):
  dataset_ret = ""

  # Add category key value pair to test, dev
  dataset["categories"] = ans

  # break into each doc
  for doc_num in range(len(dataset["idx"])):
    doc = {}
    for keys in dataset.keys():
      doc[keys] = dataset[keys][doc_num]

    # transfer from dict to str
    dataset_ret += json.dumps(doc)
    dataset_ret += "\n"

  # Write file
  with open(name + '.json', 'w') as fp:
    fp.write(dataset_ret)

# Read data file
train_data = pd.read_json('dataset/train_gold.json', lines=True)
categories = pd.read_json('dataset/categories.json', lines=True)
test_data = pd.read_json('dataset/' + name + '_unlabeled.json', lines=True)

# Manually split training and testing data
# train_data, test_data = train_test_split(data, test_size=0.1, random_state=666)

# Data augmentation by take every category of training data inro account
frame = {}                   # new pandas dataframe for augmentation data
N = 0                                 # Counter for each data row
for doc_num in range(len(train_data)):
  for cats in range(len(train_data.iloc[doc_num]["categories"])):
    frame[N] = train_data.iloc[doc_num]
    frame[N]["categories"] = [train_data.iloc[doc_num]["categories"][cats]] 
    N += 1
train_data_aug = pd.DataFrame(frame)
train_data_aug = train_data_aug.T

# Duplicate the data belongs to multiple categories
train_y = [categories[0]  for categories in train_data_aug['categories'].to_list()]

# Build model
model = BertClassifier()
reply_model = BertClassifier()

# Model options set
model.bert_model = bert_model
reply_model.bert_model = bert_model

# Model fine-tuned
model.fit(train_data_aug["text"],train_y)
reply_model.fit(train_data_aug["reply"], train_y)

# Save model
model.save(textmodel)
reply_model.save(replymodel)

# Load model
# model = load_model('textmodel')
# reply_model = load_model('replymodel')

# Prediction
pred_y = model.predict_proba(test_data["text"])
pred_reply_y = reply_model.predict_proba(test_data["reply"])

# Combine "text" and "reply" domain information
ratio = 0.5
y = np.array(pred_y)*(1 - ratio) + np.array(pred_reply_y)*ratio

# Find the 6 maximize probability categories
top_six = []
for doc_prob in y:
  index_sort = np.argsort(-np.array(doc_prob)) 
  top_six.append(list(index_sort[:6]))

for doc_idx in top_six:
  for idx in range(len(doc_idx)):
    doc_idx[idx] = categories[doc_idx[idx]][0]

dataset = {}

# convert pandas to dictionary
for key in test_data.columns:
  data_list = []
  for data_num in range(len(test_data[key])):
    data_list.append(list(test_data[key].values)[data_num])
    if isinstance(data_list[-1], np.int64):
      data_list[-1] = int(data_list[-1])

  dataset[key] = data_list[0] if len(data_list) == 1 else data_list

# write json file
write_json(dataset, top_six)

